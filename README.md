# machinelearning

# Направления методов машинного обучения: методов главных компонентов 
МГК - это уменьшение размерности с минимальными потерями в информативности 

Рассмотрим пример с таблицей:

<img width="484" alt="Снимок экрана 2024-01-10 в 15 23 54" src="https://github.com/arlinrus/machinelearning/assets/111064731/a45e8110-171c-4395-bd90-28ece3ca447d">

Для начала найдем среднее арифметичнское для каждого из объектов и произведем центрирование для объектов x1 и x2

<img width="769" alt="Снимок экрана 2024-01-10 в 15 29 06" src="https://github.com/arlinrus/machinelearning/assets/111064731/0abc8dbe-f3d2-4a51-8545-57f527df92f9">

Где новые координат векторов будут иметь данные заданные значения: 0,591 и 0,807 или тоже самое с минусами.


venv/od_1_ml.py

# Линейная регрессия(изучение с учителем)

Данные о проведенном времени в мгз
<img width="373" alt="Снимок экрана 2024-01-16 в 17 34 11" src="https://github.com/arlinrus/machinelearning/assets/111064731/c95dc953-69a9-4b77-994d-fb9b919e7be4">

Выберем переменные:

Номер наблюдения в мрдели не участвует - наблюдение

Предиктор - колво товаров(х1)

Отклик - время в мгх(y)
<img width="600" alt="Снимок экрана 2024-01-16 в 17 36 23" src="https://github.com/arlinrus/machinelearning/assets/111064731/4a49a141-03be-47bc-99df-efb57578ad01">


<img width="1052" alt="Снимок экрана 2024-01-16 в 17 40 42" src="https://github.com/arlinrus/machinelearning/assets/111064731/8fec1e08-a3e4-4355-bf96-2914ed511904">


<img width="693" alt="Снимок экрана 2024-01-16 в 17 37 36" src="https://github.com/arlinrus/machinelearning/assets/111064731/30fcbcc5-a3b3-4500-bef4-9f8efe2d7c34">

Уравнение линейной регрессии имеет следующий вид:

y = 4.06 + 0.93x1

Сколько временинам потребуеся, если мы захотим приобрести 27 товаров?
Подставим все в x1 и получим 29.17

LinearReгрессия соответствует линейной модели с коэффициентами w = (w1,…, wp), чтобы минимизировать остаточную сумму квадратов между наблюдаемыми целями в наборе данных и целями, предсказанными с помощью линейного приближения.

С точки зрения реализации, это просто обычные методы наименьших квадратов (scipy.linalg.lstsq) или неотрицательные наименьшие квадраты (scipy.optimize.nnls), завернутые в объект предиктора.







